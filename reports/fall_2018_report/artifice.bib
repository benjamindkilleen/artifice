
@article{lin_microsoft_2014,
	title = {Microsoft {COCO}: {Common} {Objects} in {Context}},
	shorttitle = {Microsoft {COCO}},
	url = {http://arxiv.org/abs/1405.0312},
	abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
	urldate = {2018-06-01},
	journal = {arXiv:1405.0312 [cs]},
	author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Bourdev, Lubomir and Girshick, Ross and Hays, James and Perona, Pietro and Ramanan, Deva and Zitnick, C. Lawrence and Dollár, Piotr},
	month = may,
	year = {2014},
	note = {arXiv: 1405.0312},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 1) updated annotation pipeline description and figures; 2) added new section describing datasets splits; 3) updated author list},
	file = {arXiv\:1405.0312 PDF:/Users/Benjamin/Zotero/storage/B2KZPIIN/Lin et al. - 2014 - Microsoft COCO Common Objects in Context.pdf:application/pdf;arXiv.org Snapshot:/Users/Benjamin/Zotero/storage/D3N5TVNA/1405.html:text/html}
}

@article{nash_topological_2015,
	title = {Topological mechanics of gyroscopic metamaterials},
	volume = {112},
	issn = {0027-8424},
	url = {http://www.pnas.org/content/112/47/14495},
	doi = {10.1073/pnas.1507413112},
	abstract = {We have built a new type of mechanical metamaterial: a “gyroscopic metamaterial” composed of rapidly spinning objects that are coupled to each other. At the edges of these materials, we find sound waves that are topologically protected (i.e. they cannot be scattered backward or into the bulk). These waves, which propagate in one direction only, are directly analogous to edge currents in quantum Hall systems. Through a mathematical model, we interpret the robustness of these edge waves in light of the subtle topological character of the bulk material. Crucially, these edge motions can be controlled by distorting the metamaterial lattice, opening new avenues for the control of sound in matter.Topological mechanical metamaterials are artificial structures whose unusual properties are protected very much like their electronic and optical counterparts. Here, we present an experimental and theoretical study of an active metamaterial—composed of coupled gyroscopes on a lattice—that breaks time-reversal symmetry. The vibrational spectrum displays a sonic gap populated by topologically protected edge modes that propagate in only one direction and are unaffected by disorder. We present a mathematical model that explains how the edge mode chirality can be switched via controlled distortions of the underlying lattice. This effect allows the direction of the edge current to be determined on demand. We demonstrate this functionality in experiment and envision applications of these edge modes to the design of one-way acoustic waveguides.},
	number = {47},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Nash, Lisa M. and Kleckner, Dustin and Read, Alismari and Vitelli, Vincenzo and Turner, Ari M. and Irvine, William T. M.},
	year = {2015},
	pages = {14495--14500}
}

@article{settles_active_nodate,
	title = {Active {Learning} {Literature} {Survey}},
	language = {en},
	author = {Settles, Burr},
	pages = {67},
	file = {Settles - Active Learning Literature Survey.pdf:/Users/Benjamin/Zotero/storage/8XM5T954/Settles - Active Learning Literature Survey.pdf:application/pdf}
}

@article{bai_deep_2016,
	title = {Deep {Watershed} {Transform} for {Instance} {Segmentation}},
	url = {http://arxiv.org/abs/1611.08303},
	abstract = {Most contemporary approaches to instance segmentation use complex pipelines involving conditional random fields, recurrent neural networks, object proposals, or template matching schemes. In our paper, we present a simple yet powerful end-to-end convolutional neural network to tackle this task. Our approach combines intuitions from the classical watershed transform and modern deep learning to produce an energy map of the image where object instances are unambiguously represented as basins in the energy map. We then perform a cut at a single energy level to directly yield connected components corresponding to object instances. Our model more than doubles the performance of the state-of-the-art on the challenging Cityscapes Instance Level Segmentation task.},
	urldate = {2018-10-19},
	journal = {arXiv:1611.08303 [cs]},
	author = {Bai, Min and Urtasun, Raquel},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.08303},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1611.08303 PDF:/Users/Benjamin/Zotero/storage/WKPTMBWF/Bai and Urtasun - 2016 - Deep Watershed Transform for Instance Segmentation.pdf:application/pdf;arXiv.org Snapshot:/Users/Benjamin/Zotero/storage/5JQQETTQ/1611.html:text/html}
}

@book{barber_bayesian_2011,
	address = {Cambridge},
	title = {Bayesian {Reasoning} and {Machine} {Learning}},
	isbn = {978-0-511-80477-9},
	url = {http://ebooks.cambridge.org/ref/id/CBO9780511804779},
	language = {en},
	urldate = {2018-10-19},
	publisher = {Cambridge University Press},
	author = {Barber, David},
	year = {2011},
	doi = {10.1017/CBO9780511804779},
	file = {Barber - 2011 - Bayesian Reasoning and Machine Learning.pdf:/Users/Benjamin/Zotero/storage/Z9P4DAL4/Barber - 2011 - Bayesian Reasoning and Machine Learning.pdf:application/pdf}
}

@article{long_fully_nodate,
	title = {Fully {Convolutional} {Networks} for {Semantic} {Segmentation}},
	abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixelsto-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build “fully convolutional” networks that take input of arbitrary size and produce correspondingly-sized output with efﬁcient inference and learning. We deﬁne and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classiﬁcation networks (AlexNet [22], the VGG net [34], and GoogLeNet [35]) into fully convolutional networks and transfer their learned representations by ﬁne-tuning [5] to the segmentation task. We then deﬁne a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, ﬁne layer to produce accurate and detailed segmentations. Our fully convolutional network achieves stateof-the-art segmentation of PASCAL VOC (20\% relative improvement to 62.2\% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one ﬁfth of a second for a typical image.},
	language = {en},
	author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
	pages = {10},
	file = {Long et al. - Fully Convolutional Networks for Semantic Segmenta.pdf:/Users/Benjamin/Zotero/storage/3N9YM7G6/Long et al. - Fully Convolutional Networks for Semantic Segmenta.pdf:application/pdf}
}

@article{ren_faster_2015,
	title = {Faster {R}-{CNN}: {Towards} {Real}-{Time} {Object} {Detection} with {Region} {Proposal} {Networks}},
	shorttitle = {Faster {R}-{CNN}},
	url = {http://arxiv.org/abs/1506.01497},
	abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
	urldate = {2018-10-19},
	journal = {arXiv:1506.01497 [cs]},
	author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
	month = jun,
	year = {2015},
	note = {arXiv: 1506.01497},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Extended tech report},
	file = {arXiv\:1506.01497 PDF:/Users/Benjamin/Zotero/storage/8HTDVZCT/Ren et al. - 2015 - Faster R-CNN Towards Real-Time Object Detection w.pdf:application/pdf;arXiv.org Snapshot:/Users/Benjamin/Zotero/storage/JE3KGFZT/1506.html:text/html}
}

@article{kao_localization-aware_2018,
	title = {Localization-{Aware} {Active} {Learning} for {Object} {Detection}},
	url = {http://arxiv.org/abs/1801.05124},
	abstract = {Active learning - a class of algorithms that iteratively searches for the most informative samples to include in a training dataset - has been shown to be effective at annotating data for image classification. However, the use of active learning for object detection is still largely unexplored as determining informativeness of an object-location hypothesis is more difficult. In this paper, we address this issue and present two metrics for measuring the informativeness of an object hypothesis, which allow us to leverage active learning to reduce the amount of annotated data needed to achieve a target object detection performance. Our first metric measures 'localization tightness' of an object hypothesis, which is based on the overlapping ratio between the region proposal and the final prediction. Our second metric measures 'localization stability' of an object hypothesis, which is based on the variation of predicted object locations when input images are corrupted by noise. Our experimental results show that by augmenting a conventional active-learning algorithm designed for classification with the proposed metrics, the amount of labeled training data required can be reduced up to 25\%. Moreover, on PASCAL 2007 and 2012 datasets our localization-stability method has an average relative improvement of 96.5\% and 81.9\% over the baseline method using classification only.},
	urldate = {2018-10-19},
	journal = {arXiv:1801.05124 [cs]},
	author = {Kao, Chieh-Chi and Lee, Teng-Yok and Sen, Pradeep and Liu, Ming-Yu},
	month = jan,
	year = {2018},
	note = {arXiv: 1801.05124},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1801.05124 PDF:/Users/Benjamin/Zotero/storage/8LRSSP5N/Kao et al. - 2018 - Localization-Aware Active Learning for Object Dete.pdf:application/pdf;arXiv.org Snapshot:/Users/Benjamin/Zotero/storage/A2WGGXZP/1801.html:text/html}
}

@inproceedings{jain_active_2016,
	title = {Active {Image} {Segmentation} {Propagation}},
	doi = {10.1109/CVPR.2016.313},
	abstract = {We propose a semi-automatic method to obtain foreground object masks for a large set of related images. We develop a stagewise active approach to propagation: in each stage, we actively determine the images that appear most valuable for human annotation, then revise the foreground estimates in all unlabeled images accordingly. In order to identify images that, once annotated, will propagate well to other examples, we introduce an active selection procedure that operates on the joint segmentation graph over all images. It prioritizes human intervention for those images that are uncertain and influential in the graph, while also mutually diverse. We apply our method to obtain foreground masks for over 1 million images. Our method yields state-of-the-art accuracy on the ImageNet and MIT Object Discovery datasets, and it focuses human attention more effectively than existing propagation strategies.},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Jain, S. D. and Grauman, K.},
	month = jun,
	year = {2016},
	keywords = {active image segmentation propagation, active selection, Computer vision, Feature extraction, feature selection, Focusing, foreground estimates, foreground object masks, graph theory, human annotation, image annotation, image segmentation, Image segmentation, ImageNet datasets, Manuals, MIT object discovery datasets, Proposals, segmentation graph, Spatial databases, stagewise active approach, unlabeled images},
	pages = {2864--2873},
	file = {IEEE Xplore Abstract Record:/Users/Benjamin/Zotero/storage/VWBWLIDT/7780682.html:text/html;IEEE Xplore Full Text PDF:/Users/Benjamin/Zotero/storage/BLXEQQIC/Jain and Grauman - 2016 - Active Image Segmentation Propagation.pdf:application/pdf}
}

@inproceedings{vezhnevets_active_2012,
	title = {Active learning for semantic segmentation with expected change},
	doi = {10.1109/CVPR.2012.6248050},
	abstract = {We address the problem of semantic segmentation: classifying each pixel in an image according to the semantic class it belongs to (e.g. dog, road, car). Most existing methods train from fully supervised images, where each pixel is annotated by a class label. To reduce the annotation effort, recently a few weakly supervised approaches emerged. These require only image labels indicating which classes are present. Although their performance reaches a satisfactory level, there is still a substantial gap between the accuracy of fully and weakly supervised methods. We address this gap with a novel active learning method specifically suited for this setting. We model the problem as a pairwise CRF and cast active learning as finding its most informative nodes. These nodes induce the largest expected change in the overall CRF state, after revealing their true label. Our criterion is equivalent to maximizing an upper-bound on accuracy gain. Experiments on two data-sets show that our method achieves 97\% percent of the accuracy of the corresponding fully supervised model, while querying less than 17\% of the (super-)pixel labels.},
	booktitle = {2012 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Vezhnevets, A. and Buhmann, J. M. and Ferrari, V.},
	month = jun,
	year = {2012},
	keywords = {Accuracy, active learning, class label, Computational modeling, expected change, fully supervised images, image label, image segmentation, Image segmentation, Labeling, learning (artificial intelligence), pairwise CRF, Roads, semantic segmentation, Semantics, Training, weakly supervised method},
	pages = {3162--3169},
	file = {IEEE Xplore Abstract Record:/Users/Benjamin/Zotero/storage/FDJACNQY/6248050.html:text/html;IEEE Xplore Full Text PDF:/Users/Benjamin/Zotero/storage/3U9YSYWM/Vezhnevets et al. - 2012 - Active learning for semantic segmentation with exp.pdf:application/pdf}
}

@article{akeret_radio_2016,
	title = {Radio frequency interference mitigation using deep convolutional neural networks},
	url = {http://arxiv.org/abs/1609.09077},
	abstract = {We propose a novel approach for mitigating radio frequency interference (RFI) signals in radio data using the latest advances in deep learning. We employ a special type of Convolutional Neural Network, the U-Net, that enables the classification of clean signal and RFI signatures in 2D time-ordered data acquired from a radio telescope. We train and assess the performance of this network using the HIDE \& SEEK radio data simulation and processing packages, as well as early Science Verification data acquired with the 7m single-dish telescope at the Bleien Observatory. We find that our U-Net implementation is showing competitive accuracy to classical RFI mitigation algorithms such as SEEK's SumThreshold implementation. We publish our U-Net software package on GitHub under GPLv3 license.},
	urldate = {2018-10-21},
	journal = {arXiv:1609.09077 [astro-ph]},
	author = {Akeret, Joel and Chang, Chihway and Lucchi, Aurelien and Refregier, Alexandre},
	month = sep,
	year = {2016},
	note = {arXiv: 1609.09077},
	keywords = {Astrophysics - Instrumentation and Methods for Astrophysics},
	annote = {Comment: 8 pages, 3 figures Published in Astronomy and Computing. The code is available at https://github.com/jakeret/tf\_unet},
	file = {arXiv\:1609.09077 PDF:/Users/Benjamin/Zotero/storage/BFXCP5RT/Akeret et al. - 2016 - Radio frequency interference mitigation using deep.pdf:application/pdf;arXiv.org Snapshot:/Users/Benjamin/Zotero/storage/TEEHFZDX/1609.html:text/html}
}

@article{chen_encoder-decoder_2018,
	title = {Encoder-{Decoder} with {Atrous} {Separable} {Convolution} for {Semantic} {Image} {Segmentation}},
	url = {http://arxiv.org/abs/1802.02611},
	abstract = {Spatial pyramid pooling module or encode-decoder structure are used in deep neural networks for semantic segmentation task. The former networks are able to encode multi-scale contextual information by probing the incoming features with filters or pooling operations at multiple rates and multiple effective fields-of-view, while the latter networks can capture sharper object boundaries by gradually recovering the spatial information. In this work, we propose to combine the advantages from both methods. Specifically, our proposed model, DeepLabv3+, extends DeepLabv3 by adding a simple yet effective decoder module to refine the segmentation results especially along object boundaries. We further explore the Xception model and apply the depthwise separable convolution to both Atrous Spatial Pyramid Pooling and decoder modules, resulting in a faster and stronger encoder-decoder network. We demonstrate the effectiveness of the proposed model on PASCAL VOC 2012 and Cityscapes datasets, achieving the test set performance of 89.0{\textbackslash}\% and 82.1{\textbackslash}\% without any post-processing. Our paper is accompanied with a publicly available reference implementation of the proposed models in Tensorflow at {\textbackslash}url\{https://github.com/tensorflow/models/tree/master/research/deeplab\}.},
	urldate = {2018-10-21},
	journal = {arXiv:1802.02611 [cs]},
	author = {Chen, Liang-Chieh and Zhu, Yukun and Papandreou, George and Schroff, Florian and Adam, Hartwig},
	month = feb,
	year = {2018},
	note = {arXiv: 1802.02611},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: ECCV 2018 camera ready},
	file = {arXiv\:1802.02611 PDF:/Users/Benjamin/Zotero/storage/LAGWSPGK/Chen et al. - 2018 - Encoder-Decoder with Atrous Separable Convolution .pdf:application/pdf;arXiv.org Snapshot:/Users/Benjamin/Zotero/storage/IDT623VC/1802.html:text/html}
}

@article{chen_semantic_2014,
	title = {Semantic {Image} {Segmentation} with {Deep} {Convolutional} {Nets} and {Fully} {Connected} {CRFs}},
	url = {http://arxiv.org/abs/1412.7062},
	abstract = {Deep Convolutional Neural Networks (DCNNs) have recently shown state of the art performance in high level vision tasks, such as image classification and object detection. This work brings together methods from DCNNs and probabilistic graphical models for addressing the task of pixel-level classification (also called "semantic image segmentation"). We show that responses at the final layer of DCNNs are not sufficiently localized for accurate object segmentation. This is due to the very invariance properties that make DCNNs good for high level tasks. We overcome this poor localization property of deep networks by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF). Qualitatively, our "DeepLab" system is able to localize segment boundaries at a level of accuracy which is beyond previous methods. Quantitatively, our method sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 71.6\% IOU accuracy in the test set. We show how these results can be obtained efficiently: Careful network re-purposing and a novel application of the 'hole' algorithm from the wavelet community allow dense computation of neural net responses at 8 frames per second on a modern GPU.},
	urldate = {2018-10-21},
	journal = {arXiv:1412.7062 [cs]},
	author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L.},
	month = dec,
	year = {2014},
	note = {arXiv: 1412.7062},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: 14 pages. Updated related work},
	file = {arXiv\:1412.7062 PDF:/Users/Benjamin/Zotero/storage/74XSH9GU/Chen et al. - 2014 - Semantic Image Segmentation with Deep Convolutiona.pdf:application/pdf;arXiv.org Snapshot:/Users/Benjamin/Zotero/storage/4GUXZ4BR/1412.html:text/html}
}

@article{liu_parsenet:_2015,
	title = {{ParseNet}: {Looking} {Wider} to {See} {Better}},
	shorttitle = {{ParseNet}},
	url = {http://arxiv.org/abs/1506.04579},
	abstract = {We present a technique for adding global context to deep convolutional networks for semantic segmentation. The approach is simple, using the average feature for a layer to augment the features at each location. In addition, we study several idiosyncrasies of training, significantly increasing the performance of baseline networks (e.g. from FCN). When we add our proposed global feature, and a technique for learning normalization parameters, accuracy increases consistently even over our improved versions of the baselines. Our proposed approach, ParseNet, achieves state-of-the-art performance on SiftFlow and PASCAL-Context with small additional computational cost over baselines, and near current state-of-the-art performance on PASCAL VOC 2012 semantic segmentation with a simple approach. Code is available at https://github.com/weiliu89/caffe/tree/fcn .},
	urldate = {2018-10-21},
	journal = {arXiv:1506.04579 [cs]},
	author = {Liu, Wei and Rabinovich, Andrew and Berg, Alexander C.},
	month = jun,
	year = {2015},
	note = {arXiv: 1506.04579},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: ICLR 2016 submission},
	file = {arXiv\:1506.04579 PDF:/Users/Benjamin/Zotero/storage/7CS94QHF/Liu et al. - 2015 - ParseNet Looking Wider to See Better.pdf:application/pdf;arXiv.org Snapshot:/Users/Benjamin/Zotero/storage/ZB58U7YZ/1506.html:text/html}
}

@inproceedings{bernardis_finding_2010,
	title = {Finding dots: {Segmentation} as popping out regions from boundaries},
	shorttitle = {Finding dots},
	doi = {10.1109/CVPR.2010.5540210},
	abstract = {Many applications need to segment out all small round regions in an image. This task of finding dots can be viewed as a region segmentation problem where the dots form one region and the areas between dots form the other. We formulate it as a graph cuts problem with two types of grouping cues: short-range attraction based on feature similarity and long-range repulsion based on feature dissimilarity. The feature we use is a pixel-centric relational representation that encodes local convexity: Pixels inside the dots and outside the dots become sinks and sources of the feature vector. Normalized cuts on both attraction and repulsion pop out all the dots in a single binary segmentation. Our experiments show that our method is more accurate and robust than state-of-art segmentation algorithms on four categories of microscopic images. It can also detect textons in natural scene images with the same set of parameters.},
	booktitle = {2010 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Bernardis, E. and Yu, S. X.},
	month = jun,
	year = {2010},
	keywords = {Auditory system, Biomedical imaging, Cancer, Educational institutions, Embryo, feature dissimilarity, feature similarity, finding dots, graph cuts problem, graph theory, image representation, image resolution, image segmentation, Image segmentation, Layout, long-range repulsion, Microscopy, object detection, pixel-centric relational representation, popping out regions, region segmentation problem, Robustness, short-range attraction, Silicon, single binary segmentation},
	pages = {199--206},
	file = {IEEE Xplore Abstract Record:/Users/Benjamin/Zotero/storage/L9MVBJ3E/5540210.html:text/html;IEEE Xplore Full Text PDF:/Users/Benjamin/Zotero/storage/EBB8XA5V/Bernardis and Yu - 2010 - Finding dots Segmentation as popping out regions .pdf:application/pdf}
}

@inproceedings{maire_affinity_2016,
	title = {Affinity {CNN}: {Learning} {Pixel}-{Centric} {Pairwise} {Relations} for {Figure}/{Ground} {Embedding}},
	shorttitle = {Affinity {CNN}},
	url = {https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Maire_Affinity_CNN_Learning_CVPR_2016_paper.html},
	urldate = {2018-11-06},
	author = {Maire, Michael and Narihira, Takuya and Yu, Stella X.},
	year = {2016},
	pages = {174--182},
	file = {Full Text PDF:/Users/Benjamin/Zotero/storage/SWTDPNL7/Maire et al. - 2016 - Affinity CNN Learning Pixel-Centric Pairwise Rela.pdf:application/pdf;Snapshot:/Users/Benjamin/Zotero/storage/USK9EBMV/Maire_Affinity_CNN_Learning_CVPR_2016_paper.html:text/html}
}

@article{zhao_pyramid_2016,
	title = {Pyramid {Scene} {Parsing} {Network}},
	url = {http://arxiv.org/abs/1612.01105},
	abstract = {Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction tasks. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields new record of mIoU accuracy 85.4\% on PASCAL VOC 2012 and accuracy 80.2\% on Cityscapes.},
	urldate = {2018-11-06},
	journal = {arXiv:1612.01105 [cs]},
	author = {Zhao, Hengshuang and Shi, Jianping and Qi, Xiaojuan and Wang, Xiaogang and Jia, Jiaya},
	month = dec,
	year = {2016},
	note = {arXiv: 1612.01105},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: CVPR 2017},
	file = {arXiv\:1612.01105 PDF:/Users/Benjamin/Zotero/storage/EAQ6VLBR/Zhao et al. - 2016 - Pyramid Scene Parsing Network.pdf:application/pdf;arXiv.org Snapshot:/Users/Benjamin/Zotero/storage/YP7AKCGM/1612.html:text/html}
}

@article{ronneberger_u-net:_2015,
	title = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
	shorttitle = {U-{Net}},
	url = {http://arxiv.org/abs/1505.04597},
	abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
	urldate = {2018-11-08},
	journal = {arXiv:1505.04597 [cs]},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	month = may,
	year = {2015},
	note = {arXiv: 1505.04597},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: conditionally accepted at MICCAI 2015},
	file = {arXiv\:1505.04597 PDF:/Users/Benjamin/Zotero/storage/6R2U6YQ4/Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image.pdf:application/pdf;arXiv.org Snapshot:/Users/Benjamin/Zotero/storage/RHCR3G5P/1505.html:text/html}
}

@article{deng_imagenet:_nodate,
	title = {{ImageNet}: {A} {Large}-{Scale} {Hierarchical} {Image} {Database}},
	abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a largescale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 5001000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classiﬁcation and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
	language = {en},
	author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
	pages = {8},
	file = {Deng et al. - ImageNet A Large-Scale Hierarchical Image Databas.pdf:/Users/Benjamin/Zotero/storage/FLRF5VCQ/Deng et al. - ImageNet A Large-Scale Hierarchical Image Databas.pdf:application/pdf}
}

@inproceedings{pathak_context_2016,
	address = {Las Vegas, NV, USA},
	title = {Context {Encoders}: {Feature} {Learning} by {Inpainting}},
	isbn = {978-1-4673-8851-1},
	shorttitle = {Context {Encoders}},
	url = {http://ieeexplore.ieee.org/document/7780647/},
	doi = {10.1109/CVPR.2016.278},
	abstract = {We present an unsupervised visual feature learning algorithm driven by context-based pixel prediction. By analogy with auto-encoders, we propose Context Encoders – a convolutional neural network trained to generate the contents of an arbitrary image region conditioned on its surroundings. In order to succeed at this task, context encoders need to both understand the content of the entire image, as well as produce a plausible hypothesis for the missing part(s). When training context encoders, we have experimented with both a standard pixel-wise reconstruction loss, as well as a reconstruction plus an adversarial loss. The latter produces much sharper results because it can better handle multiple modes in the output. We found that a context encoder learns a representation that captures not just appearance but also the semantics of visual structures. We quantitatively demonstrate the effectiveness of our learned features for CNN pre-training on classiﬁcation, detection, and segmentation tasks. Furthermore, context encoders can be used for semantic inpainting tasks, either stand-alone or as initialization for non-parametric methods.},
	language = {en},
	urldate = {2018-11-26},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Pathak, Deepak and Krahenbuhl, Philipp and Donahue, Jeff and Darrell, Trevor and Efros, Alexei A.},
	month = jun,
	year = {2016},
	pages = {2536--2544},
	file = {Pathak et al. - 2016 - Context Encoders Feature Learning by Inpainting.pdf:/Users/Benjamin/Zotero/storage/W24KWXYG/Pathak et al. - 2016 - Context Encoders Feature Learning by Inpainting.pdf:application/pdf}
}